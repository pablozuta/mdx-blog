---
title: Python Libraries for Data Science 
tags:
  - python
  - numpy
  - matplotlib
  - data science
date: 12-01-2023
excerpt: Python has gained a lot of traction over the years in the field of Data Analytics. It's easy-to-learn, free to install, and has many libraries designed for quick and easy data manipulation such as Pandas, Matplotlib, and NumPy. 
---

## NumPy
Why is it a good idea to use NumPy rather than basic Python data structures and functions?
Data can come from many different sources, but often we can consider them as _arrays_ (or grids) of numbers. For instance, an image can be seen as a two dimensional array (or matrix) where each cell represents the intensity of a pixel. Being able to efficiently handle these arrays is really important, and NumPy is what allows us to do so.
NumPy stands for **Numerical Python** and provides us with an interface for operating on numbers. From a user point of view, NumPy arrays behave similarly to Python lists. However, it is much _faster_ to operate on NumPy arrays, especially when they are large. NumPy arrays are at the foundation of the whole Python data science ecosystem.
Let's start by importing NumPy:

```python
import numpy as np
```
### Create a Numpy Array
Unlike Python lists, NumPy arrays can only hold _one specific_ type of data. The exact type of array is automatically worked out at its creation, and has an impact on the operations that can be performed on it. You can also specify the type manually. We'll see examples of each shortly.

```python
# Array of integers
np.array([1, 3, 4, 5, 8])
```
And if you want to manually set the data type:

```python
np.array([2, 5, 7, 43], dtype='float32')
```
Unlike Python lists, NumPy arrays can be explicitly multidimensional. This means that NumPy recognizes multidimensional tables (for example, a table of numbers with rows and columns).
It's often more efficient, especially for large arrays, to create them yourself. NumPy provides us with quite a few ways to do this:

```python
# An array of length 10, filled with 0:
np.zeros(10, dtype=int)
# An array of size 3x5 filled with 1.0 (float)
np.ones((3, 5), dtype=float)
# An array of size 3x5 filled with 3.14
np.full((3, 5), 3.14)
# An array containing a linear sequence starting at 0 and 
# going up to 20, with steps of 2
np.arange(0, 20, 2)
# An array of 5 numbers, linearly spaced between 0 and 1
np.linspace(0, 1, 5)
# An array of the given shape and populate it with random
# samples. You can also try using "randint" and "normal"
np.random.random((3, 3))
# The identity matrix of size 3
np.eye(3)
```
NumPy arrays have some very useful properties! For example, if you want to know how many dimensions an array has, you can use  .ndim . To confirm the dimension of a shape, you can also look at its shape, using  .shape . To look at the size of the array, or in other words, to see how many elements it has, we can use  .size. Finally, if we want to see the data type of the elements, we can look at that using  .dtype.

```python
np.random.seed(0)
x1 = np.random.randint(10, size=6)  
# 1-dimensional array
print("Number of dimensions: ", x1.ndim)
print("Shape: ", x1.shape)
print("Size: ", x1.size)
print("Type: ", x1.dtype)
```
#### Accessing a single element

You'll often need to access specific elements of an array. This is called **indexing**. Fortunately, this is really easy with NumPy!

```python
print(x1)
# The first element
print(x1[0])
# The last element
print(x1[-1])

x2 = np.random.randint(10, size=(3, 4))  
# 2-dimensional array
print(x2[0,1])

# You can also modify values:
x1[1] = "1000"
print(x1)

# Mind the type 
x1[1] = 3.14
print(x1)
```

## Matplotlib

#### Visualize Uncertainty
Another aspect of plotting is the **_degree of uncertainty associated with estimates_.**
To visualize uncertainty we are going to use what we call **error bars**.

##### Discrete Data
In the case of discrete data, we often use error bars to represent the uncertainty inherent to each point's value. Often, the length of the bars matches the _standard deviation_ of the _empirical observations_.

```python
x = np.linspace(0, 10, 50)
dy = 0.8
y = np.sin(x) + dy * np.random.randn(50)
plt.errorbar(x, y, yerr=dy, fmt='.k');
plt.show()
```

Errorbar takes as argument the x-coordinates, y-coordinates and lengths of each bar (one bar per point)  yerr . Note the  fmt  argument. It allows you to choose the color (here black) and the shape of the markers on the graph in a really concise way. Errorbar  also allows you to further customize the appearance of the graph.

```python
x = np.linspace(0, 10, 50)
dy = 0.8
y = np.sin(x) + dy * np.random.randn(50)
plt.errorbar(x, y, yerr=dy, fmt='o', color="black",
			ecolor='lightgray', elinewidth=3, capsize=0);
```

### Customize your plots
Up to this point, we have been constructing plots from the ground up, but you can actually use a set of _preconfigured_ properties called **styles**.
let's print the first six styles that we have.

```python
print(plt.style.available[:6])
fig = plt.figure(figsize=(12, 8))
for i in range(6):
    fig.add_subplot(3, 2, i + 1)
    plt.style.use(plt.style.available[i])
    plt.plot(x, y)
    plt.text(s=plt.style.available[i], x=5, y=2, color='red')
```

## Explore your data visually with Seaborn
Seaborn  is a library that improves Matplotlib's functionality, replaces some default settings and functions, and adds new features.

Seaborn was created to correct three **defects of Matplotlib.** As a standalone, Matplotlib:

-   Can't generate graphics of high aesthetic quality (especially in pre 2.0 versions).
    
-   Lacks the functionality to easily create sophisticated statistical analyses.
    
-   Features functions that aren't designed to interact with Panda Dataframes (which we will see in the next chapter).
    

Luckily, Seaborn addresses these problems! It still uses Matplotlib "under the hood", but does so by exposing more intuitive functions.

```python
import seaborn as sns
sns.set()
x = np.linspace(0, 10, 500)
y = np.random.randn(500)
plt.plot(x,y)
plt.show()
```
Seaborn also provides us with functions to generate useful plots for statistical analysis. For example,`distplot` lets you not only view the histogram of a sample, but also estimate the distribution from which the sample is derived.

```python
sns.distplot(y, kde=True);
plt.show()
```

As I mentioned, ==Seaborn is really good at visualizing relationships and helping us draw insights from our data. What we will do now is demonstrate Seaborn's capacities using a very simple dataset called "Iris". This dataset is popular in introductory stats classes.==

It contains 150 rows done on 3 different plant species. Each row is an observation of a certain species of plant. The observation contains quantitive columns including length and width of its sepals and petals.

```python
iris = sns.load_dataset('iris')
iris.head()
```
We can actually visualize the _relationship_ between all these variables using a powerful function in Seaborn called .pairplot.
Conveniently, you only need one line of code to do this with Seaborn! We simply need to pass "Iris" as the data parameter, set the hue to _species_, and the size to _2.5_.

```python
sns.pairplot(iris, hue='species', height=2.5);
plt.show()
```


----
## Manipulate data contained in DataFrames
Now that you know how to create a DataFrame, let's look at some other common data operations. To do this, I suggest you use a DataSet available in the Seaborn library! The dataset in question includes data on the survivors of the Titanic.

In this chapter, we'll follow a "typical" working session.
```python
import numpy as np
import pandas as pd
import seaborn as sns
titanic = sns.load_dataset('titanic')
```
The first thing to do is to take a quick look at our data.
```python
print(titanic.head())
```
The  tail  function is the counterpart of the  `head`  function. It displays the latest elements of the DataFrame.
```python
print(titanic.tail())
```
Now, try looking at all ages. The  `unique`  function returns the unique values present in a Pandas data structure.
```python
print(titanic.age.unique())
```
I should also mention the excellent  `describe`  function. It provides various statistics (average, maximum, minimum, etc.) on the data in each column:
```python
print(titanic.describe(include="all"))
```
You might have noticed  `NaN` values in the `describe`  function.  NaN literally stands for **Not a Number** and is used to represent a value that is undefined or unrepresentable. For example, we obtain  `NaN`  if we ask Pandas to calculate the average of a column of text.
The first is to replace  `NaN`  with other values. This operation is performed using the  `fillna`  function. Let's look at its application on the `age` column:

```python
titanic.fillna(value={"age": 0}).age.head(10)
```
We could also have filled the  `NaN`  with the previous values:
```python
titanic.fillna(method="pad").age.head(10)
```
Secondly, the  `dropna`  function let's you delete axes (columns or rows) that contain  `NaN`  . By default, it deletes the relevant lines:
```python
titanic.dropna().head(10)
```
But we can also delete the columns altogether!
```python
titanic.dropna(axis="columns").head()
```
### Renaming a column
Use the `rename`  function to rename the columns or rows of a DataFrame. You can do this in two ways.
```python
titanic.rename(columns={"sex":"gender"})
```
### Delete axes
The  `drop`  function allows you to delete axes (columns or rows) from a DataFrame.
```python
# Will delete the line with an index equal to 0.
titanic.drop(0)

# Deletes the "age" column.
titanic.drop(columns=["age"])
```
### Pivot tables
Before you get into relational algebra, let's have a look at pivot tables. You may be familiar with this concept if, for example you have used them in spreadsheet software. These tables are used to synthesize the data in a DataFrame. Again, let's use our Titanic dataset as an example.

To see the distribution of survivors by gender and ticket type, we only need one line:
```python
titanic.pivot_table('survived', index='sex', columns='class')
```
By default,  `pivot_table`  groups the data according to the criteria we specify, and aggregates the results on average. We can also specify other functions. For example, if we want to know the total number of survivors in each case, we can use the sum function.
```python
titanic.pivot_table('survived', index='sex', columns='class', aggfunc="sum")
```
Of course, this only works because the dataset's authors have wisely opted to represent survival with 0s and 1s!

`pivot_table`  is a very powerful function that also allows for multi-level aggregations. For example, we can see the age of survivors as an additional dimension. As the exact number of years is of little interest to us, we can group the ages into three categories, thanks to the  `cut`  function.
```python
titanic.dropna(inplace=True)
age = pd.cut(titanic['age'], [0, 18, 80])
titanic.pivot_table('survived', ['sex', age], 'class')
```
----
## Apply relational algebra operations on DataFrames

### Relational algebra

Relational algebra is a theory for manipulating data that's in **table form**, which is perfect because a DataFrame is a table! We use relational algebra to define the operations on tables; operations that can be  grouped into 5 main categories:

-   Projection and restriction
-   Set operations (union, difference, intersection)
-   Cartesian product
-   Join
-   Aggregation

### Pandas data structures
Pandas has two fundamental data structures, the  `Series`  and the  `DataFrame`  . These structures can be seen as a generalization of NumPy's tables and matrices.
The key difference between these and NumPy's versions is that Pandas' objects have explicit indexes.
Let's start with a reminder of how we create these structures and then use them for some basic operations.
```python
import numpy as np
import pandas as pd
# We can create a series from a lit
data = pd.Series([0.25, 0.5, 0.75, 1.0])
print("data looks like a numpy array: ", data)

# We can manually specify indexes
data = pd.Series([0.25, 0.5, 0.75, 1.0],        
                index=['a', 'b', 'c', 'd'])
print("data looks like a Python dict: ", data)

print(data['b'])
# We can create a Series directly from a dict:
population_dict = {'California': 38332521,
                    'Texas': 26448193,
                    'New York': 19651127,               
                    'Florida': 19552860,                  
                    'Illinois': 12882135}
area_dict = {'California': 423967,
            'Texas': 695662,
            'New York': 141297,
            'Florida': 170312,
            'Illinois': 149995}

population = pd.Series(population_dict)
area = pd.Series(area_dict)
print(population)
# What do you think of this line?
print(population['California':'Florida'])
```
Just as operations on NumPy arrays are faster than those on Python lists,  `Series`  operations are faster than those on dicts.

`DataFrames`  allow you to combine several  `Series`  into columns, much like in an SQL table. Building a  `DataFrame`  is easy:
```python
# From a Series
df = pd.DataFrame(population, columns=['population'])
print(df)

# From a list of dict
data = [{'a': i, 'b': 2 * i} for i in range(3)]
df = pd.DataFrame(data)
print(df)

# From several Series
df = pd.DataFrame({'population': population,
                    'area': area})
print(df)

# From a 2-dimensional Numpy array
df = pd.DataFrame(np.random.rand(3, 2),
                columns=['foo', 'bar'],
                index=['a', 'b', 'c'])
print(df)

# A function to easily generate DataFrames. It will be very 
# useful in the rest of this chapter.
def make_df(cols, ind):    
    """Quickly create DataFrames"""
    data = {c: [str(c) + str(i) for i in ind]
            for c in cols}
    return pd.DataFrame(data, ind)
    
# example
make_df('ABC', range(3))
```
### Projection and restriction
The first 2 operations of relational algebra are very simple. Projection is a selection of certain columns and restriction is a selection of certain rows.

We can refer to the elements of the Pandas objects by using either their implicit indexes (like we do with Numpy arrays) or explicit indexes (as in the dicts). To avoid confusion, it is advisable to use the  `loc`  (which refers by index) and  `iloc`  (which refers by position) attributes of each object.
```python
data = pd.Series([0.25, 0.5, 0.75, 1.0],
                index=['a', 'b', 'c', 'd'])
print(data)

print(data.loc['b'])
print(data.iloc[1])
```
The difference between the two should be clear after executing these lines. We perform these same operations on  `DataFrames`  in a similar way:
```python
data = pd.DataFrame({'area':area, 'pop':population})
print(data)
data.loc[:'Illinois', :'pop']
```

### Union
One of the simplest operations in relational algebra is data  `union` . That is uniting two tables to create a third, which contains all the rows of both the first and the second!

Here, we're interested in the union of  `Series`  or  `DataFrame`. With Pandas, this operation is carried out using `pd.concat`.
```python
ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])
ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])
pd.concat([ser1, ser2])
```
For a  `Series`, this seems easy. But for a  `DataFrame`?
```python
df1 = make_df('AB', [1, 2])
df2 = make_df('AB', [3, 4])
pd.concat([df1, df2])
```
By default,`pd.concat`  assembles its arguments "vertically". To change this orientation, we can use the  `axis`  argument.

#### Use indexes

Concatenation preserves the indexes! So if the two DataFrames given in arguments have common indexes, the final result will have **duplicate** indexes.

This is often a problem. To compensate for this, you can use hierarchical indexes.

```python
x = make_df('AB', [0, 1])
y = make_df('AB', [2, 3])
y.index = x.index  
# Same indexes

print(pd.concat([x, y]))
# We can ask for hierarchical indexes
hdf = pd.concat([x, y], keys=['x', 'y'])
print(hdf)
```

### Join

 Another very useful function to manipulate Dataframes is  `pd.merge`, which allows you to join DataFrames.

A join is assembles information from one table **A** with that from another table B according to a chosen criterion. This criterion is called the **join condition**. This condition is composed of one or more columns that are common to A and B.

Here is a quick example to help explain. Let's imagine that we have two Dataframes:

-   `df1`  containing a list of employees and the names of the departments in which they work,
-   `df2`  containing the same list of employees and their dates of entry into the company.
    

The  `pd.merge`  function allows us to transform these two Dataframes into one, containing both information.
```python
df1 = pd.DataFrame({
    'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],
    'department': ['Accounting','Engineering','Engineering', 'HR']})
                    
df2 = pd.DataFrame({
    'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],
    'date': [2004, 2008, 2012, 2014]})
    
df3 = pd.merge(df1, df2)
```
#### Cardinality

When performing a join, always pay attention to the cardinality of the relationship between A and B in order to avoid certain errors.
`pd.merge` make no distinction between these 3 cardinalities: it is used in exactly the same way in all 3 cases.
##### One-to-one

The example we looked at earlier corresponds to a **one-to-one cardinality**. Indeed, an employee works in only one department, and has only one year of employment.
```python
df1 = pd.DataFrame({
    'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],
    'department': ['Accounting', 'Engineering','Engineering', 'HR']})
df2 = pd.DataFrame({
    'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],
    'date': [2004, 2008, 2012, 2014]})
                    
df3 = pd.merge(df1, df2)
df3
```

##### One-to-Many, or many-to-one

Now we want to add another column. Each department has a head. This information is contained in a Dataframe. We want to add a column to  `df3`  to add each employees head of department.

This is a one-to-many cardinality. Indeed,  `df3`  represents employees and  `df4`  represents department heads. An employee has only one department head, but one department head can lead several employees.
```python
df4 = pd.DataFrame({'department': ['Accounting', 'Engineering', 
                                    'HR'],
                    'supervisor': ['Carly', 'Guido', 'Steve']})
pd.merge(df3, df4)
```
##### Many-to-many

To continue with our example, let's suppose we have another Dataframe containing the necessary skills needed to work in each department:
```python
df5 = pd.DataFrame({'department': ['Accounting', 'Accounting',
                                    'Engineering', 'Engineering', 
                                    'HR', 'HR'],    
                    'competence': ['math', 'spreadsheets', 'coding',
                                    'linux',
                                    'spreadsheets',
                                    'organization']})
```
#### Outer join

Let's take  `df1`  again and add Lea, an employee in the Engineering department. Let's call this new dataframe  `df6` . However, we do not add Lea to  `df2`  :
```python
df6 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa',
                                 'Sue', 'Lea'],                    
                    'department': ['Accounting', 'Engineering',
                                   'Engineering', 'HR',
                                   'Engineering']})

df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],
                    'date': [2004, 2008, 2012, 2014]})
```
### Cartesian product

We can use merge to perform another relational algebra operation, the Cartesian product.
```python
# We add a new column to df1 and df2, which always contains
# the same value, 0.
df1['key'] = 0
df2['key'] = 0

produit_cartesien = pd.merge(df1, df2, on='key')
# We drop the key column
produit_cartesien.drop('key',1, inplace=True)
```
### Aggregation
Like with NumPy arrays, we can easily perform operations on all the elements of a Series or Dataframe, such as  `sum`  or  `mean`  :
```python
rng = np.random.RandomState(42)
# A series with five random elements
ser = pd.Series(rng.rand(5))
print(ser.sum())
print(ser.mean())
```
These calculations are possible for a Dataframe, and are performed per column by default:
```python
df = pd.DataFrame({'A': rng.rand(5),
                   'B': rng.rand(5)})

# Per column
print(df.mean())
# Per row
print(df.mean(axis='columns'))
```
Pandas allows us to perform group aggregation, which similar to using the  `GROUP BY`  keyword in SQL.
```python
df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],   
                   'data1': range(6),
                   'data2': [10,11,10,9,10,10]})
print(df)
```
If we want to calculate the sum of the columns  `data1`  and  `data2`, and calculate the average of just  `data2`  alone, it is possible to select the necessary columns on our object gb:
```python
s = gb['data1','data2'].sum()
m = gb['data2',].mean()

groupped = pd.concat([s,m], axis=1)
groupped.columns = ["data1_somme","data2_somme","data2_moyenne"]
```
