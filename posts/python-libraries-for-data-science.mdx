---
title: Python Libraries for Data Science 
tags:
  - python
  - numpy
  - matplotlib
  - data science
date: 12-01-2023
excerpt: Python has gained a lot of traction over the years in the field of Data Analytics. It's easy-to-learn, free to install, and has many libraries designed for quick and easy data manipulation such as Pandas, Matplotlib, and NumPy. 
---

## NumPy
Why is it a good idea to use NumPy rather than basic Python data structures and functions?
Data can come from many different sources, but often we can consider them as _arrays_ (or grids) of numbers. For instance, an image can be seen as a two dimensional array (or matrix) where each cell represents the intensity of a pixel. Being able to efficiently handle these arrays is really important, and NumPy is what allows us to do so.
NumPy stands for **Numerical Python** and provides us with an interface for operating on numbers. From a user point of view, NumPy arrays behave similarly to Python lists. However, it is much _faster_ to operate on NumPy arrays, especially when they are large. NumPy arrays are at the foundation of the whole Python data science ecosystem.
Let's start by importing NumPy:

```python
import numpy as np
```
### Create a Numpy Array
Unlike Python lists, NumPy arrays can only hold _one specific_ type of data. The exact type of array is automatically worked out at its creation, and has an impact on the operations that can be performed on it. You can also specify the type manually. We'll see examples of each shortly.

```python
# Array of integers
np.array([1, 3, 4, 5, 8])
```
And if you want to manually set the data type:

```python
np.array([2, 5, 7, 43], dtype='float32')
```
Unlike Python lists, NumPy arrays can be explicitly multidimensional. This means that NumPy recognizes multidimensional tables (for example, a table of numbers with rows and columns).
It's often more efficient, especially for large arrays, to create them yourself. NumPy provides us with quite a few ways to do this:

```python
# An array of length 10, filled with 0:
np.zeros(10, dtype=int)
# An array of size 3x5 filled with 1.0 (float)
np.ones((3, 5), dtype=float)
# An array of size 3x5 filled with 3.14
np.full((3, 5), 3.14)
# An array containing a linear sequence starting at 0 and 
# going up to 20, with steps of 2
np.arange(0, 20, 2)
# An array of 5 numbers, linearly spaced between 0 and 1
np.linspace(0, 1, 5)
# An array of the given shape and populate it with random
# samples. You can also try using "randint" and "normal"
np.random.random((3, 3))
# The identity matrix of size 3
np.eye(3)
```
NumPy arrays have some very useful properties! For example, if you want to know how many dimensions an array has, you can use  .ndim . To confirm the dimension of a shape, you can also look at its shape, using  .shape . To look at the size of the array, or in other words, to see how many elements it has, we can use  .size. Finally, if we want to see the data type of the elements, we can look at that using  .dtype.

```python
np.random.seed(0)
x1 = np.random.randint(10, size=6)  
# 1-dimensional array
print("Number of dimensions: ", x1.ndim)
print("Shape: ", x1.shape)
print("Size: ", x1.size)
print("Type: ", x1.dtype)
```
#### Accessing a single element

You'll often need to access specific elements of an array. This is called **indexing**. Fortunately, this is really easy with NumPy!

```python
print(x1)
# The first element
print(x1[0])
# The last element
print(x1[-1])

x2 = np.random.randint(10, size=(3, 4))  
# 2-dimensional array
print(x2[0,1])

# You can also modify values:
x1[1] = "1000"
print(x1)

# Mind the type 
x1[1] = 3.14
print(x1)
```

## Matplotlib

#### Visualize Uncertainty
Another aspect of plotting is the **_degree of uncertainty associated with estimates_.**
To visualize uncertainty we are going to use what we call **error bars**.

##### Discrete Data
In the case of discrete data, we often use error bars to represent the uncertainty inherent to each point's value. Often, the length of the bars matches the _standard deviation_ of the _empirical observations_.

```python
x = np.linspace(0, 10, 50)
dy = 0.8
y = np.sin(x) + dy * np.random.randn(50)
plt.errorbar(x, y, yerr=dy, fmt='.k');
plt.show()
```

Errorbar takes as argument the x-coordinates, y-coordinates and lengths of each bar (one bar per point)  yerr . Note the  fmt  argument. It allows you to choose the color (here black) and the shape of the markers on the graph in a really concise way. Errorbar  also allows you to further customize the appearance of the graph.

```python
x = np.linspace(0, 10, 50)
dy = 0.8
y = np.sin(x) + dy * np.random.randn(50)
plt.errorbar(x, y, yerr=dy, fmt='o', color="black",
			ecolor='lightgray', elinewidth=3, capsize=0);
```

### Customize your plots
Up to this point, we have been constructing plots from the ground up, but you can actually use a set of _preconfigured_ properties called **styles**.
let's print the first six styles that we have.

```python
print(plt.style.available[:6])
fig = plt.figure(figsize=(12, 8))
for i in range(6):
    fig.add_subplot(3, 2, i + 1)
    plt.style.use(plt.style.available[i])
    plt.plot(x, y)
    plt.text(s=plt.style.available[i], x=5, y=2, color='red')
```

## Explore your data visually with Seaborn
Seaborn  is a library that improves Matplotlib's functionality, replaces some default settings and functions, and adds new features.

Seaborn was created to correct three **defects of Matplotlib.** As a standalone, Matplotlib:

-   Can't generate graphics of high aesthetic quality (especially in pre 2.0 versions).
    
-   Lacks the functionality to easily create sophisticated statistical analyses.
    
-   Features functions that aren't designed to interact with Panda Dataframes (which we will see in the next chapter).
    

Luckily, Seaborn addresses these problems! It still uses Matplotlib "under the hood", but does so by exposing more intuitive functions.

```python
import seaborn as sns
sns.set()
x = np.linspace(0, 10, 500)
y = np.random.randn(500)
plt.plot(x,y)
plt.show()
```
Seaborn also provides us with functions to generate useful plots for statistical analysis. For example,`distplot` lets you not only view the histogram of a sample, but also estimate the distribution from which the sample is derived.

```python
sns.distplot(y, kde=True);
plt.show()
```

As I mentioned, ==Seaborn is really good at visualizing relationships and helping us draw insights from our data. What we will do now is demonstrate Seaborn's capacities using a very simple dataset called "Iris". This dataset is popular in introductory stats classes.==

It contains 150 rows done on 3 different plant species. Each row is an observation of a certain species of plant. The observation contains quantitive columns including length and width of its sepals and petals.

```python
iris = sns.load_dataset('iris')
iris.head()
```
We can actually visualize the _relationship_ between all these variables using a powerful function in Seaborn called .pairplot.
Conveniently, you only need one line of code to do this with Seaborn! We simply need to pass "Iris" as the data parameter, set the hue to _species_, and the size to _2.5_.

```python
sns.pairplot(iris, hue='species', height=2.5);
plt.show()
```


----
## Manipulate data contained in DataFrames
Now that you know how to create a DataFrame, let's look at some other common data operations. To do this, I suggest you use a DataSet available in the Seaborn library! The dataset in question includes data on the survivors of the Titanic.

In this chapter, we'll follow a "typical" working session.
```python
import numpy as np
import pandas as pd
import seaborn as sns
titanic = sns.load_dataset('titanic')
```
The first thing to do is to take a quick look at our data.
```python
print(titanic.head())
```
The  tail  function is the counterpart of the  `head`  function. It displays the latest elements of the DataFrame.
```python
print(titanic.tail())
```
Now, try looking at all ages. The  `unique`  function returns the unique values present in a Pandas data structure.
```python
print(titanic.age.unique())
```
I should also mention the excellent  `describe`  function. It provides various statistics (average, maximum, minimum, etc.) on the data in each column:
```python
print(titanic.describe(include="all"))
```
You might have noticed  `NaN` values in the `describe`  function.  NaN literally stands for **Not a Number** and is used to represent a value that is undefined or unrepresentable. For example, we obtain  `NaN`  if we ask Pandas to calculate the average of a column of text.
The first is to replace  `NaN`  with other values. This operation is performed using the  `fillna`  function. Let's look at its application on the `age` column:

```python
titanic.fillna(value={"age": 0}).age.head(10)
```
We could also have filled the  `NaN`  with the previous values:
```python
titanic.fillna(method="pad").age.head(10)
```
Secondly, the  `dropna`  function let's you delete axes (columns or rows) that contain  `NaN`  . By default, it deletes the relevant lines:
```python
titanic.dropna().head(10)
```
But we can also delete the columns altogether!
```python
titanic.dropna(axis="columns").head()
```
### Renaming a column
Use the `rename`  function to rename the columns or rows of a DataFrame. You can do this in two ways.
```python
titanic.rename(columns={"sex":"gender"})
```
### Delete axes
The  `drop`  function allows you to delete axes (columns or rows) from a DataFrame.
```python
# Will delete the line with an index equal to 0.
titanic.drop(0)

# Deletes the "age" column.
titanic.drop(columns=["age"])
```
### Pivot tables
Before you get into relational algebra, let's have a look at pivot tables. You may be familiar with this concept if, for example you have used them in spreadsheet software. These tables are used to synthesize the data in a DataFrame. Again, let's use our Titanic dataset as an example.

To see the distribution of survivors by gender and ticket type, we only need one line:
```python
titanic.pivot_table('survived', index='sex', columns='class')
```
By default,  `pivot_table`  groups the data according to the criteria we specify, and aggregates the results on average. We can also specify other functions. For example, if we want to know the total number of survivors in each case, we can use the sum function.
```python
titanic.pivot_table('survived', index='sex', columns='class', aggfunc="sum")
```
Of course, this only works because the dataset's authors have wisely opted to represent survival with 0s and 1s!

`pivot_table`  is a very powerful function that also allows for multi-level aggregations. For example, we can see the age of survivors as an additional dimension. As the exact number of years is of little interest to us, we can group the ages into three categories, thanks to the  `cut`  function.
```python
titanic.dropna(inplace=True)
age = pd.cut(titanic['age'], [0, 18, 80])
titanic.pivot_table('survived', ['sex', age], 'class')
```

